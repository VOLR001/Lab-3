{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488b8266",
   "metadata": {},
   "source": [
    "# Lab Assignment: Building a Neural Network from Scratch\n",
    "\n",
    "### Objective\n",
    "This lab guides you through the implementation of a simple feedforward neural network from scratch. \n",
    "By completing this lab, you will:\n",
    "- Initialize a neural network with weights and biases.\n",
    "- Compute the weighted sum at each node.\n",
    "- Apply activation functions for node outputs.\n",
    "- Perform forward propagation to compute predictions.\n",
    "- Implement backpropagation to compute gradients.\n",
    "- Update weights using gradients to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668a14e",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "396df339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole Manchester + Initialized Network: {'weights1': array([[ 0.00496714, -0.00138264,  0.00647689],\n",
      "       [ 0.0152303 , -0.00234153, -0.00234137],\n",
      "       [ 0.01579213,  0.00767435, -0.00469474],\n",
      "       [ 0.0054256 , -0.00463418, -0.0046573 ]]), 'biases1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'weights2': array([[ 0.00241962, -0.0191328 , -0.01724918, -0.00562288],\n",
      "       [-0.01012831,  0.00314247, -0.00908024, -0.01412304],\n",
      "       [ 0.01465649, -0.00225776,  0.00067528, -0.01424748]]), 'biases2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'weights3': array([[-0.00544383,  0.00110923, -0.01150994]]), 'biases3': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) # For reproducibility\n",
    "def initialize_network(input_size, hidden_layers, output_size):\n",
    "    # Your code should contain the weights and biases\n",
    "    # provide your code\n",
    "    network = {}\n",
    "    layer_sizes = [input_size] + hidden_layers + [output_size]  # Combine all layer sizes\n",
    "\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.01  # Xavier/Glorot initialization for sigmoid\n",
    "        biases = np.zeros((layer_sizes[i+1], 1))\n",
    "\n",
    "        network[f'weights{i+1}'] = weights # weights matrix for the layer\n",
    "        network[f'biases{i+1}'] = biases # bias vector for the layer\n",
    "\n",
    "    return network\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "# Initialize a network with 3 inputs, 2 hidden layers (4 and 3 nodes), and 1 output node\n",
    "network = initialize_network(3, [4, 3], 1)\n",
    "print(\"Cole Manchester + Initialized Network:\", network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfdd84",
   "metadata": {},
   "source": [
    "## Step 2: Compute Weighted Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "66c4ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole + Weighted Sum: [[-1.00200758]\n",
      " [ 0.55444362]\n",
      " [-0.03492299]\n",
      " [-0.15251886]]\n"
     ]
    }
   ],
   "source": [
    "def compute_weighted_sum(inputs, weights, biases):\n",
    "    # Please numpy dot to calcuate the compute weighted with linear \n",
    "    # provide your code\n",
    "    if inputs.ndim == 1:  \n",
    "        inputs = inputs.reshape(-1, 1)  # Ensure column vector\n",
    "    \n",
    "    elif inputs.shape[0] == 1:  \n",
    "        inputs = inputs.T  # Convert row vector (1,3) â†’ (3,1)\n",
    "    \n",
    "    return np.dot(weights, inputs) + biases \n",
    "    #return np.sum(inputs * weights) + biases\n",
    "    #use numpy dot function\n",
    "    \n",
    "    \n",
    "network = {\n",
    "    'weights': np.random.randn(4, 3),  # 4 neurons, 3 input features\n",
    "    'biases': np.zeros((4, 1))         # 4 neurons, 1 bias per neuron\n",
    "}#test for the sum values\n",
    "\n",
    "# Test weighted sum\n",
    "inputs = np.array([[0.5, 0.2, 0.1]])\n",
    "layer = network   #set as dictionary so no [0] needed  # First layer\n",
    "Z = compute_weighted_sum(inputs, layer['weights'], layer['biases'])\n",
    "print(\"Cole + Weighted Sum:\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82553f",
   "metadata": {},
   "source": [
    "## Step 3: Compute Node Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "36b00bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole + Activation: [[0.26854689]\n",
      " [0.63516593]\n",
      " [0.49127014]\n",
      " [0.46194403]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(Z):\n",
    "#     provide your code\n",
    "    return 1/(1+np.exp(-Z))\n",
    " \n",
    "def sigmoid_derivative(A):\n",
    "    # provide your code\n",
    "    return A*(1-A)\n",
    "\n",
    "# Compute activation for the weighted sum\n",
    "A = sigmoid(Z)\n",
    "print(\"Cole + Activation:\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f9c55",
   "metadata": {},
   "source": [
    "## Step 4: Perform Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5d2e9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer 1 ---\n",
      "Current Input Shape: (3, 1)\n",
      "Weights Shape: (4, 3)\n",
      "Biases Shape: (4, 1)\n",
      "Z (Weighted Sum) Shape: (4, 1)\n",
      "A (Activations) Shape: (4, 1)\n",
      "--- Layer 2 ---\n",
      "Current Input Shape: (4, 1)\n",
      "Weights Shape: (2, 4)\n",
      "Biases Shape: (2, 1)\n",
      "Z (Weighted Sum) Shape: (2, 1)\n",
      "A (Activations) Shape: (2, 1)\n",
      "[array([[0.5],\n",
      "       [0.2],\n",
      "       [0.1]]), array([[0.57716488],\n",
      "       [0.38718866],\n",
      "       [0.52699662],\n",
      "       [0.30447332]]), array([[0.56506212],\n",
      "       [0.39580708]])]\n",
      "Cole + Final Output: [[0.56506212]\n",
      " [0.39580708]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(inputs, network):\n",
    "   # performs forward propagation through the network\n",
    "    inputs = np.array(inputs)  # convert to numpy array if not already\n",
    "    if inputs.shape[0] == 1:\n",
    "        inputs = inputs.T  # convert to column vector if single example\n",
    "\n",
    "    activations = [inputs]\n",
    "    current_input = inputs\n",
    "    layer_num = 1\n",
    "\n",
    "    while f'weights{layer_num}' in network and f'biases{layer_num}' in network:\n",
    "        print(f\"--- Layer {layer_num} ---\")  # indicate the layer number -- \n",
    "\n",
    "        weights = network[f'weights{layer_num}']\n",
    "        biases = network[f'biases{layer_num}']\n",
    "\n",
    "        print(\"Current Input Shape:\", current_input.shape)\n",
    "        print(\"Weights Shape:\", weights.shape)\n",
    "        print(\"Biases Shape:\", biases.shape)\n",
    "\n",
    "        z = compute_weighted_sum(current_input, weights, biases)\n",
    "        print(\"Z (Weighted Sum) Shape:\", z.shape)\n",
    "\n",
    "        a = sigmoid(z)\n",
    "        print(\"A (Activations) Shape:\", a.shape)\n",
    "\n",
    "        activations.append(a)\n",
    "        current_input = a  # update for next layer\n",
    "        layer_num += 1\n",
    "\n",
    "    return activations\n",
    "\n",
    "# Example input & network info\n",
    "inputs = np.array([[0.5, 0.2, 0.1]]) # 1 input, 3 features\n",
    "network = {\n",
    "    'weights1': np.random.randn(4, 3),\n",
    "    'biases1': np.zeros((4, 1)),\n",
    "    'weights2': np.random.randn(2, 4),\n",
    "    'biases2': np.zeros((2, 1))\n",
    "}\n",
    "\n",
    "\n",
    "# Perform forward propagation\n",
    "activations = forward_propagation(inputs, network)\n",
    "print(activations)\n",
    "print(\"Cole + Final Output:\", activations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342f58d",
   "metadata": {},
   "source": [
    "## Step 5: Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1ec4d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole + Gradients: {'weights1': array([[-0.03304352,  0.05761835,  2.46193618],\n",
      "       [-0.18822995,  0.30319975, -0.03388557],\n",
      "       [-1.1471225 ,  1.15144503,  0.75624414],\n",
      "       [ 0.7938667 , -0.90825355,  1.40336126]]), 'biases1': array([[-0.0130593 ],\n",
      "       [ 0.00826203],\n",
      "       [ 0.04311107],\n",
      "       [ 0.00566951]]), 'weights2': array([[-1.36483401,  0.61168983,  2.22425508, -0.97100862],\n",
      "       [-0.59907659,  0.0776618 , -0.53340532, -1.56795535]]), 'biases2': array([[ 0.06413601],\n",
      "       [-0.05679289]])}\n"
     ]
    }
   ],
   "source": [
    "def backpropagation(network, activations, y_true):\n",
    "    learning_rate = 0.6\n",
    "    gradients = {}  # Store gradients for weight and bias updates\n",
    "    m = Y.shape[1]  # Number of training examples\n",
    "    dA = activations[-1] - Y  # Output layer error\n",
    "\n",
    "    for layer in reversed(range(1, len(activations))):\n",
    "        A_prev = activations[layer - 1]\n",
    "        W = network[f'weights{layer}']\n",
    "        \n",
    "        dZ = dA * sigmoid_derivative(activations[layer])  # apply derivative\n",
    "        dW = np.dot(dZ, A_prev.T) / m  # weight gradient\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m  # bias gradient\n",
    "        # store gradients\n",
    "        gradients[f'dW{layer}'] = dW\n",
    "        gradients[f'db{layer}'] = db\n",
    "        # update weights and biases\n",
    "        network[f'weights{layer}'] -= learning_rate * dW\n",
    "        network[f'biases{layer}'] -= learning_rate * db\n",
    "\n",
    "        # cmpute error for previous layer\n",
    "        dA = np.dot(W.T, dZ)  # Backpropagate error\n",
    "\n",
    "    return network\n",
    "\n",
    "# Example setup\n",
    "Y = np.array([[1], [0]])  # True labels (2 neurons output)\n",
    "network = {\n",
    "    'weights1': np.random.randn(4, 3),\n",
    "    'biases1': np.zeros((4, 1)),\n",
    "    'weights2': np.random.randn(2, 4),\n",
    "    'biases2': np.zeros((2, 1))\n",
    "}\n",
    "\n",
    "# Perform forward propagation\n",
    "inputs = np.array([[0.5, 0.2, 0.1]])\n",
    "# Compute gradients\n",
    "y_true = np.array([[1]])  # Example target output\n",
    "gradients = backpropagation(network, activations, y_true)\n",
    "print(\"Cole + Gradients:\", gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b81797-9f82-493a-a509-b1360f04071c",
   "metadata": {},
   "source": [
    "## Step 6: Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "33990e4e-9a02-4728-9ff5-fc41e7ef42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Network: {'layer1': {'weights': array([[0.094, 0.188],\n",
      "       [0.282, 0.376]]), 'biases': array([[0.47 ],\n",
      "       [0.564]])}, 'layer2': {'weights': array([[0.658, 0.752],\n",
      "       [0.846, 0.94 ]]), 'biases': array([[1.034],\n",
      "       [1.128]])}}\n",
      "Cole + Updated Network: {'layer1': {'weights': array([[0.093, 0.186],\n",
      "       [0.279, 0.372]]), 'biases': array([[0.465],\n",
      "       [0.558]])}, 'layer2': {'weights': array([[0.651, 0.744],\n",
      "       [0.837, 0.93 ]]), 'biases': array([[1.023],\n",
      "       [1.116]])}}\n"
     ]
    }
   ],
   "source": [
    "def update_weights(network, gradients, learning_rate):\n",
    "    # Hints: weights -= learning_rate * 'dW'\n",
    "    # Hints: biases -= learning_rate * 'db'\n",
    "    # Provide your code \n",
    "    for layer in range(1, len(network) + 1):  # iterate through layers\n",
    "        network[f'layer{layer}']['weights'] -= learning_rate * gradients[f'dW{layer}']\n",
    "        network[f'layer{layer}']['biases'] -= learning_rate * gradients[f'db{layer}']\n",
    "\n",
    "# test Network\n",
    "network = {\n",
    "    'layer1': {'weights': np.array([[0.1, 0.2], [0.3, 0.4]]), 'biases': np.array([[0.5], [0.6]])},\n",
    "    'layer2': {'weights': np.array([[0.7, 0.8], [0.9, 1.0]]), 'biases': np.array([[1.1], [1.2]])}\n",
    "}\n",
    "\n",
    "gradients = {\n",
    "    'dW1': np.array([[0.01, 0.02], [0.03, 0.04]]),\n",
    "    'db1': np.array([[0.05], [0.06]]),\n",
    "    'dW2': np.array([[0.07, 0.08], [0.09, 0.10]]),\n",
    "    'db2': np.array([[0.11], [0.12]])\n",
    "}\n",
    "\n",
    "learning_rate = 0.6\n",
    "update_weights(network, gradients, learning_rate)\n",
    "print(\"Updated Network:\", network)\n",
    "\n",
    "\n",
    "\n",
    "# Update weights with a learning rate of 0.1\n",
    "update_weights(network, gradients, learning_rate=0.1)\n",
    "print(\"Cole + Updated Network:\", network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55add1",
   "metadata": {},
   "source": [
    "## Step 7: Visualizing Loss Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9fe355dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.028000000000000032\n",
      "MSE Loss: 0.028000000000000032\n"
     ]
    }
   ],
   "source": [
    "# Use MSE to compute the loss \n",
    "import numpy as np\n",
    "def compute_loss(y_true, y_pred):\n",
    "    # provide your code\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_pred = np.array(y_pred).reshape(-1)\n",
    "    #back to numpy\n",
    "    return np.mean((y_true - y_pred) ** 2)  # Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "\n",
    "# a good test case:\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.1, 1.8, 3.2, 3.9, 5.2]\n",
    "\n",
    "loss = compute_loss(y_true, y_pred)\n",
    "print(f\"MSE Loss: {loss}\")\n",
    "\n",
    "\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1.1, 1.8, 3.2, 3.9, 5.2])\n",
    "\n",
    "loss = compute_loss(y_true, y_pred)\n",
    "print(f\"MSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f7270921",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[231], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m backpropagation(activations, y_true, network)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#should give us a gradient!\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     update_weights(network, gradients, learning_rate)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Plot Loss and rerun all cells\u001b[39;00m\n\u001b[0;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "Cell \u001b[1;32mIn[226], line 6\u001b[0m, in \u001b[0;36mupdate_weights\u001b[1;34m(network, gradients, learning_rate)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_weights\u001b[39m(network, gradients, learning_rate):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Hints: weights -= learning_rate * 'dW'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Hints: biases -= learning_rate * 'db'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Provide your code \u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(network) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# iterate through layers\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         network[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m         network[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiases\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradients[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Training Loop\n",
    "losses = []\n",
    "inputs = np.array([[0.5, 0.2, 0.1]])\n",
    "y_true = np.array([[1]])\n",
    "learning_rate = 0.1\n",
    "\n",
    "for iteration in range(100):\n",
    "    # provide your code\n",
    "    # Hints: forward_propagation function with inputs network\n",
    "    #        compute_loss for y_true and activations[-1]\n",
    "    #        add loss to losses\n",
    "     \n",
    "\n",
    "    # gradients = backpropagation function\n",
    "    # update_weights\n",
    "    activations = forward_propagation(inputs,network)\n",
    "    y_pred = activations[-1]\n",
    "    loss = compute_loss(y_true, y_pred)\n",
    "    #prediction is the final output \n",
    "    losses.append(loss)\n",
    "    gradients = backpropagation(activations, y_true, network)\n",
    "    #should give us a gradient!\n",
    "    update_weights(network, gradients, learning_rate)\n",
    "    \n",
    "# Plot Loss and rerun all cells\n",
    "plt.plot(losses)\n",
    "plt.title(\"Cole + Loss Before and After Weight Updates\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f4b49-9f2b-4e5c-8f75-cb102fc858e0",
   "metadata": {},
   "source": [
    "### Step 8: Visualizing Gradients Changes (Graduate students)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a89b54-adeb-4a5d-89b2-1883ce9634a7",
   "metadata": {},
   "source": [
    "Please pick a weight and plot the gradient change\n",
    "\n",
    "You need to point which weight you pick and label it on your graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e59f30cb-09da-48e9-9551-d9055db6e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0d830-3ed2-4d96-9a72-3cca41792c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee45b1-2541-4f77-8962-be11e1d268f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a6c6b-752f-49bd-9d88-80d4e9601916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
