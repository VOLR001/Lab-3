{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488b8266",
   "metadata": {},
   "source": [
    "# Lab Assignment: Building a Neural Network from Scratch\n",
    "\n",
    "### Objective\n",
    "This lab guides you through the implementation of a simple feedforward neural network from scratch. \n",
    "By completing this lab, you will:\n",
    "- Initialize a neural network with weights and biases.\n",
    "- Compute the weighted sum at each node.\n",
    "- Apply activation functions for node outputs.\n",
    "- Perform forward propagation to compute predictions.\n",
    "- Implement backpropagation to compute gradients.\n",
    "- Update weights using gradients to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668a14e",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396df339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole Manchester + Initialized Network: {'weights1': array([[ 0.49671415, -0.1382643 ,  0.64768854],\n",
      "       [ 1.52302986, -0.23415337, -0.23413696],\n",
      "       [ 1.57921282,  0.76743473, -0.46947439],\n",
      "       [ 0.54256004, -0.46341769, -0.46572975]]), 'biases1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'weights2': array([[ 0.24196227, -1.91328024, -1.72491783, -0.56228753],\n",
      "       [-1.01283112,  0.31424733, -0.90802408, -1.4123037 ],\n",
      "       [ 1.46564877, -0.2257763 ,  0.0675282 , -1.42474819]]), 'biases2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'weights3': array([[-0.54438272,  0.11092259, -1.15099358]]), 'biases3': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) # For reproducibility\n",
    "def initialize_network(input_size, hidden_layers, output_size):\n",
    "    # Your code should contain the weights and biases\n",
    "    # provide your code\n",
    "    network = {}\n",
    "    layer_sizes = [input_size] + hidden_layers + [output_size]  #combine all layers\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        network[f'weights{i+1}'] = np.random.randn(layer_sizes[i+1], layer_sizes[i])  #random weights\n",
    "        network[f'biases{i+1}'] = np.zeros((layer_sizes[i+1], 1))  #bias initialized to zero\n",
    "    \n",
    "    return network\n",
    " \n",
    "    \n",
    "\n",
    "# Initialize a network with 3 inputs, 2 hidden layers (4 and 3 nodes), and 1 output node\n",
    "network = initialize_network(3, [4, 3], 1)\n",
    "print(\"Cole Manchester + Initialized Network:\", network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfdd84",
   "metadata": {},
   "source": [
    "## Step 2: Compute Weighted Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c4ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole + Weighted Sum: [[ 0.0385519   0.06825261 -0.48643085 -0.42032083]\n",
      " [ 0.0385519   0.06825261 -0.48643085 -0.42032083]\n",
      " [ 0.0385519   0.06825261 -0.48643085 -0.42032083]\n",
      " [ 0.0385519   0.06825261 -0.48643085 -0.42032083]]\n"
     ]
    }
   ],
   "source": [
    "def compute_weighted_sum(inputs, weights, biases):\n",
    "    # Please numpy dot to calcuate the compute weighted with linear \n",
    "    # provide your code\n",
    "    return np.dot(inputs, weights.T) + biases\n",
    "    #use numpy dot function\n",
    "    \n",
    "    \n",
    "network = {\n",
    "    'weights': np.random.randn(4, 3),  # 4 neurons, 3 input features\n",
    "    'biases': np.zeros((4, 1))         # 4 neurons, 1 bias per neuron\n",
    "}#test for the sum values\n",
    "\n",
    "# Test weighted sum\n",
    "inputs = np.array([[0.5, 0.2, 0.1]])\n",
    "layer = network   #set as dictionary so no [0] needed  # First layer\n",
    "Z = compute_weighted_sum(inputs, layer['weights'], layer['biases'])\n",
    "print(\"Cole + Weighted Sum:\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82553f",
   "metadata": {},
   "source": [
    "## Step 3: Compute Node Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36b00bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cole + Activation: [[0.50963678 0.51705653 0.38073473 0.39643998]\n",
      " [0.50963678 0.51705653 0.38073473 0.39643998]\n",
      " [0.50963678 0.51705653 0.38073473 0.39643998]\n",
      " [0.50963678 0.51705653 0.38073473 0.39643998]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(Z):\n",
    "#     provide your code\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(A):\n",
    "    # provide your code\n",
    "    return A*(1-A)\n",
    "\n",
    "# Compute activation for the weighted sum\n",
    "A = sigmoid(Z)\n",
    "print(\"Cole + Activation:\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f9c55",
   "metadata": {},
   "source": [
    "## Step 4: Perform Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d2e9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.5, 0.2, 0.1]])]\n",
      "Cole + Final Output: [[0.5 0.2 0.1]]\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation(inputs, network):\n",
    "    activations = [inputs]  \n",
    "    current_input = inputs\n",
    "    layer_num = 1  #  tracks the layer number\n",
    "    while f'weights{layer_num}' in network and f'biases{layer_num}' in network:\n",
    "        weights = network[f'weights{layer_num}']\n",
    "        biases = network[f'biases{layer_num}']\n",
    "        Z = compute_weighted_sum(current_input, weights, biases)\n",
    "        A = sigmoid(Z)  #our activation funct\n",
    "        activations.append(A)\n",
    "        current_input = A\n",
    "        layer_num += 1  # increment layer number\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "\n",
    "# Perform forward propagation\n",
    "activations = forward_propagation(inputs, network)\n",
    "print(activations)\n",
    "print(\"Cole + Final Output:\", activations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342f58d",
   "metadata": {},
   "source": [
    "## Step 5: Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1ec4d75a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m]])  \u001b[38;5;66;03m# Example target output\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m gradients \u001b[38;5;241m=\u001b[39m backpropagation(network, activations, y_true)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCole + Gradients:\u001b[39m\u001b[38;5;124m\"\u001b[39m, gradients)\n",
      "Cell \u001b[1;32mIn[188], line 7\u001b[0m, in \u001b[0;36mbackpropagation\u001b[1;34m(network, activations, y_true)\u001b[0m\n\u001b[0;32m      5\u001b[0m delta \u001b[38;5;241m=\u001b[39m (y_true \u001b[38;5;241m-\u001b[39m activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m sigmoid_derivative(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      6\u001b[0m layer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m gradients[layer_name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mdot(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mT, delta), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(num_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     10\u001b[0m     layer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(network\u001b[38;5;241m.\u001b[39mkeys())[l]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def backpropagation(network, activations, y_true):\n",
    "    gradients = {}\n",
    "    num_layers = len(network)\n",
    "\n",
    "    delta = (y_true - activations[-1]) * sigmoid_derivative(activations[-1])\n",
    "    layer_name = list(network.keys())[-1]\n",
    "    gradients[layer_name] = {'dW': np.dot(activations[-2].T, delta), 'db': np.sum(delta, axis=0, keepdims=True)}\n",
    "\n",
    "    for l in reversed(range(num_layers - 1)):\n",
    "        layer_name = list(network.keys())[l]\n",
    "        delta = np.dot(delta, network[list(network.keys())[l+1]]['weights'].T) * sigmoid_derivative(activations[l+1])\n",
    "\n",
    "        print(\"Shape of activations[l].T:\", activations[l].T.shape)  # Debug print\n",
    "        print(\"Shape of delta:\", delta.shape)                     # Debug print\n",
    "        gradients[layer_name] = {'dW': np.dot(activations[l].T, delta), 'db': np.sum(delta, axis=0, keepdims=True)}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "# Compute gradients\n",
    "y_true = np.array([[1]])  # Example target output\n",
    "gradients = backpropagation(network, activations, y_true)\n",
    "print(\"Cole + Gradients:\", gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b81797-9f82-493a-a509-b1360f04071c",
   "metadata": {},
   "source": [
    "## Step 6: Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "33990e4e-9a02-4728-9ff5-fc41e7ef42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Network: {'layer1': {'weights': [[0.099, 0.198], [0.297, 0.396]], 'biases': [0.495, 0.594]}, 'layer2': {'weights': [[0.693, 0.792], [0.891, 0.99]], 'biases': [1.0890000000000002, 1.188]}}\n",
      "Cole + Updated Network: {'layer1': {'weights': [[0.098, 0.196], [0.294, 0.392]], 'biases': [0.49, 0.588]}, 'layer2': {'weights': [[0.6859999999999999, 0.784], [0.882, 0.98]], 'biases': [1.0780000000000003, 1.176]}}\n"
     ]
    }
   ],
   "source": [
    "def update_weights(network, gradients, learning_rate):\n",
    "    # Hints: weights -= learning_rate * 'dW'\n",
    "    # Hints: biases -= learning_rate * 'db'\n",
    "    # Provide your code \n",
    "   for layer_name in network:\n",
    "        if 'weights' in network[layer_name]:\n",
    "            network[layer_name]['weights'] = np.array(network[layer_name]['weights'])  # convert to numpy array\n",
    "            gradients[layer_name]['dW'] = np.array(gradients[layer_name]['dW']) # numpy array\n",
    "            network[layer_name]['weights'] -= learning_rate * gradients[layer_name]['dW']\n",
    "            network[layer_name]['weights'] = network[layer_name]['weights'].tolist() # make it back to a list\n",
    "        if 'biases' in network[layer_name]:\n",
    "            network[layer_name]['biases'] = np.array(network[layer_name]['biases']) # convert to numpy array\n",
    "            gradients[layer_name]['db'] = np.array(gradients[layer_name]['db']) # Convert to numpy array\n",
    "            network[layer_name]['biases'] -= learning_rate * gradients[layer_name]['db']\n",
    "            network[layer_name]['biases'] = network[layer_name]['biases'].tolist() # convert back to a list another time\n",
    "\n",
    "\n",
    "#my network I made up\n",
    "network = {\n",
    "    'layer1': {'weights': [[0.1, 0.2], [0.3, 0.4]], 'biases': [0.5, 0.6]},\n",
    "    'layer2': {'weights': [[0.7, 0.8], [0.9, 1.0]], 'biases': [1.1, 1.2]}\n",
    "}\n",
    "\n",
    "gradients = {\n",
    "    'layer1': {'dW': [[0.01, 0.02], [0.03, 0.04]], 'db': [0.05, 0.06]},\n",
    "    'layer2': {'dW': [[0.07, 0.08], [0.09, 0.10]], 'db': [0.11, 0.12]}\n",
    "}\n",
    "\n",
    "learning_rate = 0.1\n",
    "update_weights(network, gradients, learning_rate)\n",
    "print(\"Updated Network:\", network)\n",
    "\n",
    "\n",
    "\n",
    "# Update weights with a learning rate of 0.1\n",
    "update_weights(network, gradients, learning_rate=0.1)\n",
    "print(\"Cole + Updated Network:\", network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55add1",
   "metadata": {},
   "source": [
    "## Step 7: Visualizing Loss Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9fe355dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.028000000000000032\n",
      "MSE Loss: 0.028000000000000032\n"
     ]
    }
   ],
   "source": [
    "# Use MSE to compute the loss \n",
    "def compute_loss(y_true, y_pred):\n",
    "    # provide your code\n",
    "    y_true = np.array(y_true) #numpy conversions\n",
    "    y_pred = np.array(y_pred) \n",
    "    \n",
    "    #calculate MSE using numpy mean funct \n",
    "    mse = np.mean((y_true - y_pred)**2)  # or np.square(y_true - y_pred).mean() since they both work!\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "# a good test case:\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1.1, 1.8, 3.2, 3.9, 5.2]\n",
    "\n",
    "loss = compute_loss(y_true, y_pred)\n",
    "print(f\"MSE Loss: {loss}\")\n",
    "\n",
    "\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1.1, 1.8, 3.2, 3.9, 5.2])\n",
    "\n",
    "loss = compute_loss(y_true, y_pred)\n",
    "print(f\"MSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f7270921",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#prediction is the final output \u001b[39;00m\n\u001b[0;32m     22\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m---> 23\u001b[0m gradients \u001b[38;5;241m=\u001b[39m backpropagation(activities, y_true, network)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#should give us a gradient!\u001b[39;00m\n\u001b[0;32m     25\u001b[0m update_weights(network, gradients, learning_state)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'activities' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Training Loop\n",
    "losses = []\n",
    "inputs = np.array([[0.5, 0.2, 0.1]])\n",
    "y_true = np.array([[1]])\n",
    "learning_rate = 0.1\n",
    "\n",
    "for iteration in range(100):\n",
    "    # provide your code\n",
    "    # Hints: forward_propagation function with inputs network\n",
    "    #        compute_loss for y_true and activations[-1]\n",
    "    #        add loss to losses\n",
    "     \n",
    "\n",
    "    # gradients = backpropagation function\n",
    "    # update_weights\n",
    "    activations = forward_propagation(inputs,network)\n",
    "    y_pred = activations[-1]\n",
    "    loss = compute_loss(y_true, y_pred)\n",
    "    #prediction is the final output \n",
    "    losses.append(loss)\n",
    "    gradients = backpropagation(activities, y_true, network)\n",
    "    #should give us a gradient!\n",
    "    update_weights(network, gradients, learning_state)\n",
    "    \n",
    "# Plot Loss and rerun all cells\n",
    "plt.plot(losses)\n",
    "plt.title(\"Cole + Loss Before and After Weight Updates\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f4b49-9f2b-4e5c-8f75-cb102fc858e0",
   "metadata": {},
   "source": [
    "### Step 8: Visualizing Gradients Changes (Graduate students)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a89b54-adeb-4a5d-89b2-1883ce9634a7",
   "metadata": {},
   "source": [
    "Please pick a weight and plot the gradient change\n",
    "\n",
    "You need to point which weight you pick and label it on your graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f30cb-09da-48e9-9551-d9055db6e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0d830-3ed2-4d96-9a72-3cca41792c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
